# ðŸŽ¯ Bootstrap Step 4: Production RAG Architecture - STARTED

**Date:** October 31, 2025  
**Status:** ðŸš§ In Progress (Foundation Complete)  
**Objective:** Implement production-grade RAG with hybrid retrieval, tiered LLM routing, and intelligent caching

---

## Progress Summary

### âœ… Completed (Today)

1. **Architecture Documentation** (`BOOTSTRAP_STEP4_RAG_ARCHITECTURE.md`)
   - Complete system design (13 sections, 800+ lines)
   - Request flow diagram (normalize â†’ cache â†’ retrieve â†’ route â†’ generate â†’ validate)
   - Ingestion pipeline spec (chunking, embeddings, indexing)
   - Two-layer cache strategy (L0 exact + L1 semantic)
   - Hybrid retrieval (vector + BM25 + RRF merge)
   - Tiered routing (L1/L2/L3 with deterministic rules)
   - Output validation & guardrails
   - Failure handling & fallbacks
   - Azure service bindings
   - Observability requirements

2. **Core Type Definitions** (`types/rag.ts`)
   - API contracts: `QARequest`, `QAResponse`, `Citation`
   - Document & chunk types with metadata
   - Retrieval types: `RetrievalContext`, `HybridSearchConfig`
   - Query understanding: `QueryIntent`, `Entity`, `IntentType`
   - Routing signals: `RoutingSignals`, `TierConfig`
   - Cache types: `CacheEntry`, `SemanticCacheEntry`, `CacheStrategy`
   - Validation types: `GroundingResult`, `PIIDetectionResult`
   - Error types: `RAGError`, `RetrievalError`, `GenerationError`
   - 400+ lines of production-grade TypeScript

3. **Cache Utilities** (`lib/rag/cache-utils.ts`)
   - Query normalization & hashing (SHA-256)
   - Cache key generation (L0 exact + L1 semantic)
   - TTL strategies with jitter (prevent thundering herd)
   - Serialization/deserialization
   - Cosine similarity calculation
   - Semantic cache similarity search
   - Cache metrics collector (hit rates, latency)
   - Invalidation pattern builders
   - 350+ lines with full implementation

**Commit:** `0825326` - "Bootstrap Step 4: Production RAG architecture - types and cache utils"

---

## Architecture Highlights

### 1. Request Flow (Runtime)

```
Client â†’ POST /api/qa
  â†“
[Normalize + Rate Limit] â†’ Redis check
  â†“
[Cache Lookup]
  L0: Exact match (hash-based, 6-24h TTL)
  L1: Semantic match (vector similarity â‰¥0.92)
  â†“
HIT? â†’ Return cached answer
  â†“
MISS â†“
[Query Understanding] â†’ Intent, entities, complexity
  â†“
[Hybrid Retrieval]
  - Vector search (K=24, cosine)
  - BM25 full-text (K=24)
  - Facet filters (year, carrier)
  â†’ RRF merge â†’ Top 12
  â†’ Re-rank â†’ Top 8
  â†“
[Pattern Router] â†’ Tier selection
  Signals: coverage, evidence, tools, risk, complexity
  L1: Simple FAQ (gpt-4o-mini, 1.5s, 6h cache)
  L2: Multi-doc synthesis (gpt-4-turbo, 3s, 12h cache)
  L3: Complex/tools (gpt-4, 6s, 24h cache)
  â†“
[LLM Generate] â†’ System prompt + context + citations
  â†“
[Validation & Guardrails]
  - Grounding check (â‰¥70% sentences mapped)
  - PII/PHI redaction
  - Citation verification
  - Escalate L2â†’L3 if validation fails
  â†“
[Cache Write] â†’ TTL by tier, with jitter
  â†“
Return response
```

### 2. Ingestion Pipeline (Offline)

```
Source Docs (PDF, DOCX, HTML, JSON, FAQs)
  â†“
[Text Extraction] â†’ Per-type parsers
  â†“
[Clean & Normalize] â†’ Unicode, whitespace, headers
  â†“
[Chunking] â†’ Sliding window
  - Window: 800â€“1,200 tokens
  - Stride: 120â€“200 tokens
  - Preserve section headers
  - Attach: doc_id, section_path, position
  â†“
[Embeddings] â†’ text-embedding-3-large
  - Batch: 16â€“32 chunks
  - Retry with exponential backoff
  â†“
[Index to Azure AI Search]
  - Vector field: HNSW, cosine, 1536 dims
  - Keyword fields: title, headings, body_ngrams
  - Filters: company_id, benefit_year, carrier
```

### 3. Tier Selection Logic

| Tier | Model | Use Case | Max Tokens | Timeout | Cache TTL |
|------|-------|----------|------------|---------|-----------|
| **L1** | `gpt-4o-mini` | FAQ-like, high confidence, single chunk | 1,200 | 1.5s | 6h |
| **L2** | `gpt-4-turbo` | Multi-chunk synthesis, moderate reasoning | 2,400 | 3s | 12h |
| **L3** | `gpt-4` | Complex, tools, calculations, low coverage | 4,000 | 6s | 24h |

**Routing Signals:**
- Query length, operators, tool requirements
- Coverage: % query terms in top-k
- Evidence score: Max confidence of best chunk
- Risk score: HR/compliance keywords
- Complexity: Multi-hop reasoning needed

**Deterministic Rules:**
```typescript
// L1: Simple
if (coverage > 0.85 && evidenceScore > 0.9 && !needsTools && queryLength < 200 && riskScore < 0.3)
  return "L1";

// L3: Complex
if (needsTools || coverage < 0.5 || riskScore > 0.7 || complexityScore > 0.8)
  return "L3";

// L2: Default (moderate)
return "L2";
```

### 4. Validation & Guardrails

**Grounding Check:**
- Split answer into sentences
- Map each sentence to retrieved chunks (semantic similarity > 0.75)
- Pass if â‰¥70% sentences mapped
- Escalate tier if validation fails

**PII/PHI Redaction:**
- Regex patterns: SSN, credit card, email, phone
- Lightweight NER for medical IDs
- Redact before cache write

**Citation Verification:**
- Ensure all cited `chunkId`s were in context set
- Error if citation references chunk not retrieved

---

## Implementation Status

### Phase 1: Foundation âœ… COMPLETE
- âœ… Architecture documentation (800+ lines)
- âœ… Type definitions (400+ lines)
- âœ… Cache utilities (350+ lines)
- âœ… Key generation, TTL strategies, similarity search
- âœ… Cache metrics collector

### Phase 2: Query Understanding (Next Up)
- â¬œ Query normalization (advanced)
- â¬œ Intent detection (heuristics + patterns)
- â¬œ Entity extraction (regex + lightweight NER)
- â¬œ Complexity scoring
- â¬œ Tool detection (math, table, API calls)

### Phase 3: Hybrid Retrieval
- â¬œ Azure AI Search client wrapper
- â¬œ Vector search implementation
- â¬œ BM25 full-text search
- â¬œ RRF (Reciprocal Rank Fusion) merge
- â¬œ Cross-encoder re-ranking (optional)
- â¬œ Context builder with token budgets

### Phase 4: Routing & Generation
- â¬œ Routing signal calculation
- â¬œ Tier selection logic
- â¬œ Model picker (per tier)
- â¬œ System prompt builder
- â¬œ LLM generation wrapper
- â¬œ Streaming support

### Phase 5: Validation
- â¬œ Grounding checker
- â¬œ PII/PHI detector & redactor
- â¬œ Citation validator
- â¬œ Tier escalation logic

### Phase 6: Main API
- â¬œ `/api/qa` endpoint
- â¬œ Request validation & normalization
- â¬œ Rate limiting
- â¬œ Cache lookup (L0 + L1)
- â¬œ Orchestration flow
- â¬œ Error handling & fallbacks
- â¬œ Response formatting

### Phase 7: Ingestion Pipeline
- â¬œ Document parsers (PDF, DOCX, HTML, JSON)
- â¬œ Text extraction & cleaning
- â¬œ Sliding window chunking
- â¬œ Batch embedding generation
- â¬œ Azure Search index management
- â¬œ Ingestion API endpoint

### Phase 8: Observability
- â¬œ Application Insights integration
- â¬œ Per-tier metrics (latency, cost, errors)
- â¬œ Cache hit rate tracking
- â¬œ Retrieval coverage metrics
- â¬œ Missing intent logging
- â¬œ Health check endpoint

---

## File Manifest

### Created (Committed)
- âœ… `BOOTSTRAP_STEP4_RAG_ARCHITECTURE.md` - Complete system design
- âœ… `types/rag.ts` - All TypeScript interfaces and types
- âœ… `lib/rag/cache-utils.ts` - Cache key generation and utilities

### Pending Creation
- â¬œ `lib/rag/query-understanding.ts` - Intent detection, entity extraction
- â¬œ `lib/rag/retrieval.ts` - Hybrid search (vector + BM25 + RRF)
- â¬œ `lib/rag/router.ts` - Tier selection logic
- â¬œ `lib/rag/generation.ts` - LLM wrapper with prompt building
- â¬œ `lib/rag/validation.ts` - Grounding, PII redaction, citations
- â¬œ `lib/rag/chunking.ts` - Sliding window chunker
- â¬œ `lib/rag/embedding.ts` - Batch embedding generation
- â¬œ `lib/rag/ingestion.ts` - Document parsers and pipeline
- â¬œ `app/api/qa/route.ts` - Main QA endpoint
- â¬œ `app/api/ingest/route.ts` - Ingestion endpoint
- â¬œ `app/api/health/route.ts` - Enhanced health check

---

## Azure Service Requirements

### Azure AI Search
**Index Schema:** `chunks_prod_v1`
- Fields: `chunk_id`, `doc_id`, `company_id`, `section_path`, `content`, `content_vector` (1536 dims)
- Vector config: HNSW, cosine, m=4, efConstruction=400
- Filters: `benefit_year`, `carrier`, `doc_type`

**Status:** â¬œ Index not yet created

### Redis Cache
**Key Patterns:**
- L0: `qa:v1:{companyId}:{queryHash}`
- L1: `recentq:v1:{companyId}`
- Rate limit: `ratelimit:{userId}:{window}`

**Status:** âœ… Connection string in `.env.production`

### Azure OpenAI
**Deployments Needed:**
- L1: `gpt-4o-mini` (fast, cheap)
- L2: `gpt-4-turbo` (moderate)
- L3: `gpt-4` (complex)
- Embedding: `text-embedding-ada-002` or `text-embedding-3-large`

**Status:** â¬œ Need to verify/create deployments

### Cosmos DB (Optional)
**Containers:**
- `documents` (pk: `/companyId`) - Doc registry, ACLs
- `chunks` (pk: `/docId`) - Chunk metadata

**Status:** âœ… Connection string available, schema TBD

---

## Next Actions (Prioritized)

### Immediate (This Session)
1. **Create Azure AI Search Index**
   - Run script to create `chunks_prod_v1` with vector config
   - Test vector search with sample embedding
   - Verify HNSW performance

2. **Implement Query Understanding Module**
   - Intent detection (FAQ vs comparison vs calculation)
   - Entity extraction (plan names, dates, amounts)
   - Complexity scoring heuristics
   - Tool detection logic

3. **Build Hybrid Retrieval System**
   - Wrap Azure Search client with retry/circuit breaker
   - Implement vector + BM25 parallel queries
   - RRF merge function
   - Context builder with token budgets

### Short-Term (Next Session)
4. **Routing & Generation**
   - Signal calculation from retrieval results
   - Tier selection implementation
   - LLM generation wrapper (with Azure OpenAI)
   - Prompt templates for system/user messages

5. **Validation Layer**
   - Grounding checker with sentence mapping
   - PII/PHI regex patterns + redaction
   - Citation verification
   - Tier escalation on validation failure

### Medium-Term (This Week)
6. **Main QA API Endpoint**
   - `/api/qa` route with full orchestration
   - Rate limiting integration
   - Cache lookup (L0 + L1)
   - Error handling & degradation

7. **Ingestion Pipeline**
   - Document parser selection (pdf-parse, mammoth, cheerio)
   - Sliding window chunker
   - Batch embedding with retry
   - Azure Search bulk upload

8. **Observability**
   - Application Insights custom events
   - Metrics dashboard queries
   - Health check with service status

---

## Technical Decisions Made

1. **Cache Strategy:** Two-layer (L0 exact + L1 semantic) with tier-based TTL
2. **Retrieval:** Hybrid (vector + BM25) with RRF merge, optional cross-encoder
3. **Routing:** Deterministic rules first, ML later if needed
4. **Validation:** Grounding check â‰¥70%, tier escalation on failure
5. **Timeouts:** Aggressive (1.5s/3s/6s) with fallback degradation
6. **Chunking:** Sliding window 800-1200 tokens, stride 120-200
7. **Embeddings:** text-embedding-3-large (or Azure ada-002)
8. **Vector Index:** Azure AI Search HNSW (not separate vector DB)

---

## Open Questions

1. **Cross-Encoder Re-ranking:** Deploy separate model or use Azure AI Search semantic ranking?
2. **BM25 Implementation:** Azure Search built-in vs local Lunr.js for fallback?
3. **Semantic Cache Storage:** Redis Vector extension or small Azure Search index?
4. **Document ACLs:** Store in Cosmos or embed in Azure Search metadata?
5. **Streaming Responses:** SSE implementation for L3 queries?

---

## Dependencies & Prerequisites

**Azure Resources (from Step 3):**
- âœ… Azure AI Search: `benefits-chatbot-search`
- âœ… Redis Cache: `benefits-chatbot-redis-dev`
- âœ… Azure OpenAI: `benefits-chatbot-openai2`
- âœ… Cosmos DB: `benefits-chatbot-cosmos-dev`
- âœ… Storage: `benefitschatbotdev`

**NPM Packages Needed:**
- â¬œ `@azure/search-documents` - Azure AI Search client
- â¬œ `pdf-parse` or `pdfjs-dist` - PDF text extraction
- â¬œ `mammoth` - DOCX to HTML/text
- â¬œ `cheerio` - HTML parsing
- â¬œ `tiktoken` - Token counting (OpenAI)
- â¬œ `zod` - Runtime validation
- â¬œ `applicationinsights` - Telemetry (already have)

**Environment Variables:**
- âœ… All Azure connection strings from Step 3
- â¬œ `AZURE_OPENAI_DEPLOYMENT_L1` (gpt-4o-mini)
- â¬œ `AZURE_OPENAI_DEPLOYMENT_L2` (gpt-4-turbo)
- â¬œ `AZURE_OPENAI_DEPLOYMENT_L3` (gpt-4)
- â¬œ `AZURE_OPENAI_EMBEDDING_DEPLOYMENT`
- â¬œ `AZURE_SEARCH_INDEX_NAME` (chunks_prod_v1)

---

## Success Criteria (Step 4 Complete)

- [ ] All TypeScript types defined âœ… (DONE)
- [ ] Cache utilities implemented âœ… (DONE)
- [ ] Query understanding module working
- [ ] Hybrid retrieval functional (vector + BM25 + RRF)
- [ ] Tier routing logic implemented
- [ ] Validation & guardrails operational
- [ ] Main `/api/qa` endpoint deployed
- [ ] Ingestion pipeline functional
- [ ] Observability metrics flowing
- [ ] Test suite passing (unit + integration)
- [ ] Documentation complete
- [ ] Build passing without errors

---

**Status:** Foundation Complete (3/11 modules)  
**Next:** Query Understanding + Hybrid Retrieval  
**Timeline:** 1-2 weeks to production-ready RAG system

---

**Created by:** GitHub Copilot  
**Date:** October 31, 2025  
**Branch:** main  
**Commit:** `0825326`
